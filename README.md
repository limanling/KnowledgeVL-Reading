# Knowledge-Driven Vision-Language Pretraining Reading

This repository contains a list of papers, codes, datasets, leaderboards on the topic of **Knowledge-Driven Vision-Language Pretraining**. If you found any error, please don't hesitate to open an issue or pull request.

-- We will continue to add and update related papers and codes on this page (Update on **Jun 08th, 2022**).

## Basic vision-langauge pretraining papers and codes
(For new learners, some important papers for general NLG/KENLG.)

- **[CLIP] Learning Transferable Visual Models From Natural Language Supervision**, in PMLR 2021. [\[pdf\]](https://arxiv.org/abs/2103.00020) [\[code\]](https://github.com/openai/CLIP)

- **[Transformer] Attention Is All You Need**, in NeurIPS 2017. [\[pdf\]](https://arxiv.org/abs/1706.03762) 

## Vision-langauge pretraining: entity knowledge

- **Video-and-Language Pre-training with Entity Prompts**, in CVPR 2022. [\[pdf\]](https://arxiv.org/abs/2112.09583)

- **KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation**, in NAACL 2022 Findings. [\[pdf\]](https://arxiv.org/abs/2109.10504)

- **VinVL: Revisiting Visual Representations in Vision-Language Models**, in CVPR 2021. [\[pdf\]](https://arxiv.org/abs/2101.00529)

- **E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning**, in ACL 2021. [\[pdf\]](https://arxiv.org/abs/2106.01804)

- **Oscar: Object-semantics aligned pre-training for vision-language tasks**, in ECCV 2020. [\[pdf\]](https://arxiv.org/abs/2004.06165)

- **Uniter: Learning universal image-text representations**, in ECCV 2020. [\[pdf\]](https://arxiv.org/abs/1909.11740)

- **Vl-bert: Pre-training of generic visuallinguistic representations**, in ICLR 2020. [\[pdf\]](https://arxiv.org/abs/1908.08530)

- **Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training**, in AAAI 2020. [\[pdf\]](https://arxiv.org/abs/1908.06066)

- **Unified vision language pre-training for image captioning and VQA**, in AAAI 2020. [\[pdf\]](https://arxiv.org/abs/1909.11059)

- **ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks**, in NeurIPS 2019. [\[pdf\]](https://arxiv.org/abs/1908.02265)

- **LXMERT: Learning Cross-Modality Encoder Representations from Transformers**, in EMNLP 2019. [\[pdf\]](https://arxiv.org/abs/1908.07490)

- **Fusion of Detected Objects in Text for Visual Question Answering**, in EMNLP 2019. [\[pdf\]](https://arxiv.org/abs/1908.05054)

- **What value do explicit high level concepts have in vision to language problems?**, in CVPR 2016. [\[pdf\]](https://arxiv.org/abs/1506.01144)

- **Image Captioning with Semantic Attention**, in CVPR 2016. [\[pdf\]](https://arxiv.org/abs/1603.03925)



## Vision-langauge pretraining: relational knowledge

- **ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph**, in AAAI 2021. [\[pdf\]](https://arxiv.org/abs/2006.16934)

- **MUREL: Multimodal relational reasoning for visual question answering**, in CVPR 2019. [\[pdf\]](https://arxiv.org/abs/1902.09487)

- **Relation-aware graph attention network for visual question answering**, in ICCV 2019. [\[pdf\]](https://arxiv.org/abs/1903.12314)

- **Learning Conditioned Graph Structures for Interpretable Visual Question Answering**, in NeurIPS 2018. [\[pdf\]](https://arxiv.org/abs/1806.07243)



## Vision-langauge pretraining: event knowledge

- **CLIP-Event: Connecting Text and Images with Event Structures**, in CVPR 2022. [\[pdf\]](https://arxiv.org/abs/2201.05078)

- **Probing Image-Language Transformers for Verb Understanding**, in ACL 2021 Findings. [\[pdf\]](https://arxiv.org/abs/2106.09141)

- **ActBERT: Learning Global-Local Video-Text Representations**, in CVPR 2020. [\[pdf\]](https://arxiv.org/abs/2011.07231)

- **Multimodal understanding and reasoning for role labeling of entities in hateful memes**, in Constraint@ACL2022. [\[pdf\]](https://aclanthology.org/2022.constraint-1.2/)



## Vision-langauge pretraining: procedural knowledge

- **Learning To Recognize Procedural Activities with Distant Supervision**, in CVPR 2022. [\[pdf\]](https://arxiv.org/abs/2201.10990)

- **MERLOT Reserve: Neural Script Knowledge from Vision and Language and Sound**, in CVPR 2022. [\[pdf\]](https://arxiv.org/abs/2201.02639)

- **MERLOT: Multimodal Neural Script Knowledge Models**, in NeurIPS 2021. [\[pdf\]](https://arxiv.org/abs/2106.02636)

- **End-to-end Generative Pretraining for Multimodal Video Captioning**, in arXiv. [\[pdf\]](https://arxiv.org/abs/2201.08264)

- **Zero-Shot Anticipation for Instructional Activities**, in ICCV 2019. [\[pdf\]](https://arxiv.org/abs/1812.02501)


## Vision-langauge pretraining: Language Model (LM) knowledge


- **Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners**, in arXiv. [\[pdf\]](https://arxiv.org/pdf/2205.10747)

- **ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models**, in arXiv. [\[pdf\]](https://arxiv.org/abs/2204.08790)

- **UniT: Multimodal Multitask Learning with a Unified Transformer**, in arXiv. [\[pdf\]](https://arxiv.org/abs/2102.10772)

- **Visual Commonsense in Pretrained Unimodal and Multimodal Models**, in NAACL 2022. [\[pdf\]](https://arxiv.org/abs/2205.01850)

- **VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer**, in NeurIPS 2021. [\[pdf\]](https://arxiv.org/abs/2107.02681)

- **VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs?**, in CVPR 2021. [\[pdf\]](https://arxiv.org/abs/2101.12059)

- **M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training**, in CVPR 2021. [\[pdf\]](https://arxiv.org/abs/2006.02635)

- **Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training**, in NeurIPS 2021. [\[pdf\]](https://arxiv.org/abs/2106.13488)

- **Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision**, in EMNLP 2020. [\[pdf\]](https://arxiv.org/abs/2010.06775)



## Vision-langauge pretraining: external knowledge

- **KRIT: Knowledge-Reasoning Intelligence in vision-language Transformer**. [\[pdf\]](https://www.microsoft.com/en-us/research/uploads/prod/2022/05/KRIT.pdf)

- **KVL-BERT: Knowledge enhanced visual-and-linguistic bert for visual commonsense reasoning**, in Knowledge-Based Systems (2022). [\[pdf\]](https://arxiv.org/abs/2012.07000)


